{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL.pynb\n",
    "\n",
    "Notebook that fetches the latest measurement records from the message bus and loads\n",
    "those records into the warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import functools\n",
    "import io\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "import confluent_kafka as kafka\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# The convention with PySpark is to import individual classes, Java-style\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pyspark\n",
    "\n",
    "# Function that configures the notebook kernel by putting the lib directory \n",
    "# onto the library path and changing the working directory to the top-level\n",
    "# project dir. Idempotent.\n",
    "def setup_kernel():\n",
    "    # Move to project root if we're not already there\n",
    "    if os.getcwd().endswith(\"notebooks\"):\n",
    "        os.chdir(\"..\")\n",
    "    # TODO: Verify that we're actually at the project root.\n",
    "    # Add the scripts dir to the Python path if it's not already there.\n",
    "    scripts_dir = os.getcwd() + \"/scripts\"\n",
    "    if scripts_dir not in sys.path:\n",
    "        sys.path.append(scripts_dir)\n",
    "\n",
    "setup_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve configuration parameters from the environment, if present.\n",
    "\n",
    "# Start a map from environment var => default value.\n",
    "PARAMS_MAP = {\n",
    "    \"spark_master\": \"local[*]\",\n",
    "    \"kafka_bootstrap_servers\": \"localhost:9092\",\n",
    "    \"kafka_topic\": \"reefer\",\n",
    "    \"batch_temp_loc\": \"batch.csv\",\n",
    "    \"postgres_host\" : None,\n",
    "    \"postgres_port\" : None,\n",
    "    \"postgres_db\" : \"demo\",\n",
    "    \"postgres_user\" : None,\n",
    "    \"postgres_password\" : None,\n",
    "    \"table_name\": \"reefer_telemetries\"\n",
    "}\n",
    "\n",
    "# Override with environment variable values where applicable.\n",
    "# Uppercase names, i.e. PARAMS_MAP[\"my_var_name\"] <=> os.environ[\"MY_VAR_NAME\"]\n",
    "for k in PARAMS_MAP.keys():\n",
    "    env_var_name = k.upper()\n",
    "    if env_var_name in os.environ:\n",
    "        PARAMS_MAP[k] = os.environ[env_var_name]\n",
    "      \n",
    "# TODO: Remove the following line to avoid leaking credentials to the log\n",
    "PARAMS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire up PySpark.\n",
    "if \"spark\" not in locals(): # Make this cell idempotent\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(PARAMS_MAP[\"spark_master\"]) \\\n",
    "        .appName(\"ReeferETL\") \\\n",
    "        .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "PARAMS = spark.sparkContext.broadcast(PARAMS_MAP)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up subroutines and make sure we have a fresh copy\n",
    "import importlib\n",
    "import etl_lib\n",
    "importlib.reload(etl_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_lib.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark needs to know about every library we call\n",
    "spark.sparkContext.addPyFile(etl_lib.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our UDFs. This must happen AFTER starting Spark.\n",
    "fetch_udf = pandas_udf(etl_lib.fetch_udf,                        \n",
    "                       \"partition_id long, offset long, value string\",\n",
    "                       PandasUDFType.GROUPED_MAP)\n",
    "\n",
    "# This one has to be done differently so that it can get parameters from\n",
    "# the broadcast variable.\n",
    "@pandas_udf(\"partition_id long, offset long\", PandasUDFType.GROUPED_MAP)\n",
    "def load_udf(records):\n",
    "    return etl_lib.load_udf(records, PARAMS.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe for job control.\n",
    "partition_ids = etl_lib.get_partition_ids(PARAMS.value[\"kafka_bootstrap_servers\"],\n",
    "                                          PARAMS.value[\"kafka_topic\"])    \n",
    "params_df = spark.createDataFrame(\n",
    "    [(p, PARAMS.value[\"kafka_bootstrap_servers\"], \n",
    "      PARAMS.value[\"kafka_topic\"])\n",
    "      for p in partition_ids], \n",
    "    [\"partition_id\", \"bootstrap_servers\", \"topic_name\"])\n",
    "params_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all available messages from all partitions in parallel and write \n",
    "# the resulting messages to a temp directory on the distributed filesystem.\n",
    "# Use CSV format for ease of debugging.\n",
    "raw_batch_df = (\n",
    "    params_df\n",
    "    # Work around Spark's tendancy to use spark.sql.shuffle.partitions blindly\n",
    "    .repartition(len(partition_ids), \"partition_id\")\n",
    "    .groupby(\"partition_id\")\n",
    "    .apply(fetch_udf)\n",
    ")\n",
    "raw_batch_df.write.csv(path=PARAMS.value[\"batch_temp_loc\"], \n",
    "                       mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the temp file in a dataframe for all subsequent processing.\n",
    "batch_df = spark.read.csv(PARAMS.value[\"batch_temp_loc\"], header=True, \n",
    "                          schema=raw_batch_df.schema)\n",
    "\"{}: count = {}\".format(batch_df, batch_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk-load the batch of data into the warehouse in parallel\n",
    "num_batches = 4\n",
    "load_results = (\n",
    "    batch_df\n",
    "    # Work around Spark's tendancy to use spark.sql.shuffle.partitions blindly\n",
    "    .withColumn(\"partition_id\", batch_df.offset % num_batches)\n",
    "    .repartition(num_batches, \"partition_id\")\n",
    "    .groupby(\"partition_id\")\n",
    "    .apply(load_udf)\n",
    "    .toPandas()\n",
    ")\n",
    "load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the offsets we should commit.\n",
    "# In Kafka, you commit with the *next* offset you *will* consume, not the \n",
    "# last offset you have consumed.\n",
    "offsets_df = batch_df.groupby(\"partition_id\").agg({\"offset\": \"max\"})\n",
    "offsets_df = offsets_df.select(offsets_df[\"partition_id\"], \n",
    "                               (offsets_df[\"max(offset)\"] + 1).alias(\"to_commit\"))\n",
    "offsets_pd = offsets_df.toPandas()\n",
    "offsets_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_lib.commit_offsets(list(offsets_pd.to_records(index=False)),\n",
    "                       PARAMS.value[\"kafka_bootstrap_servers\"],\n",
    "                       PARAMS.value[\"kafka_topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
